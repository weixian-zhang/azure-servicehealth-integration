[
    {
        "id": "Microsoft.ResourceHealth/events/NKRF-1TG",
        "name": "NKRF-1TG",
        "type": "Microsoft.ResourceHealth/events",
        "properties": {
            "eventType": "ServiceIssue",
            "eventSource": "ServiceHealth",
            "status": "Resolved",
            "title": "Post Incident Review (PIR) - Azure Resource Manager - Services impacted by ARM failures ",
            "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>",
            "description": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
            "platformInitiated": true,
            "header": "Your service might have been impacted by an Azure service issue",
            "level": "Warning",
            "eventLevel": "Warning",
            "externalIncidentId": "460946878",
            "impactStartTime": "2024-01-21T01:59:30.643Z",
            "impactMitigationTime": "2024-01-21T15:15:51Z",
            "impact": [
                {
                    "impactedService": "Azure Relay",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Canada Central",
                            "status": "Active",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Canada East",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "East US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "East US 2",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "France Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Germany West Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Israel Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Italy North",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "North Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "North Europe",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Norway East",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Norway West",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Poland Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Qatar Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "South Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Southeast Asia",
                            "status": "Active",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "3/4/2024 13:02:35",
                            "updates": [
                                {
                                    "summary": "aaaaa",
                                    "updateDateTime":"3/4/2024 13:02:35"
                                },
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Sweden Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Sweden South",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Switzerland North",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "UK South",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West Europe",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US 2",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US 3",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        }
                    ]
                },
                {
                    "impactedService": "Event Hubs",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Canada Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "3/3/2024 11:02:35",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Canada East",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "East US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "East US 2",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "France Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Germany West Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Israel Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Italy North",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "North Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "North Europe",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Norway East",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Norway West",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Poland Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Qatar Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "South Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Southeast Asia",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "3/4/2024 14:02:36",
                            "updates": [
                                {
                                    "summary": "new updated 3",
                                    "updateDateTime": "3/4/2024 14:02:36"
                                },
                                {
                                    "summary": "new updated 2",
                                    "updateDateTime": "3/4/2024 14:02:35"
                                },
                                {
                                    "summary": "new updated 1",
                                    "updateDateTime": "3/3/2024 14:02:35"
                                },
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Sweden Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Sweden South",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Switzerland North",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "UK South",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West Europe",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US 2",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US 3",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        }
                    ]
                },
                {
                    "impactedService": "Service Bus",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Canada Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Canada East",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "East US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "East US 2",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "France Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Germany West Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Israel Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Italy North",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "North Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "North Europe",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Norway East",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Norway West",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Poland Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Qatar Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "South Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Southeast Asia",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Sweden Central",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Sweden South",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "Switzerland North",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "UK South",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West Central US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West Europe",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US 2",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        {
                            "impactedRegion": "West US 3",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        }
                    ]
                }
            ],
            "recommendedActions": {},
            "isHIR": false,
            "priority": 18,
            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z"
        }
    },

    {
        "id": "Microsoft.ResourceHealth/events/NKRF-1TG",
        "name": "NKRF-1TG",
        "type": "Microsoft.ResourceHealth/events",
        "properties": {
            "eventType": "ServiceIssue",
            "eventSource": "ServiceHealth",
            "status": "Resolved",
            "title": "Post Incident Review (PIR) - Azure Resource Manager - Services impacted by ARM failures ",
            "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>",
            "description": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
            "platformInitiated": true,
            "header": "Your service might have been impacted by an Azure service issue",
            "level": "Warning",
            "eventLevel": "Warning",
            "externalIncidentId": "460946878",
            "impactStartTime": "2024-01-21T01:59:30.643Z",
            "impactMitigationTime": "2024-01-21T15:15:51Z",
            "impact": [
                {
                    "impactedService": "Azure Service Bus",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Southeast Asia",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        
                        {
                            "impactedRegion": "West US 3",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        }
                    ]
                },

                {
                    "impactedService": "Azure Redis Cache",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Southeast Asia",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        },
                        
                        {
                            "impactedRegion": "West US 3",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z",
                            "updates": [
                                {
                                    "summary": "<p><em>Join our upcoming 'Azure Incident Retrospective' livestream about this incident: </em><a href=\"https://aka.ms/AIR8.1/reg\" target=\"_blank\"><em>https://aka.ms/AIR8.1/reg</em></a><em>&nbsp;</em></p>\n<p><strong>What happened?&nbsp;</strong></p>\n<p>Between 01:30 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. While the impact was predominantly experienced in Central US, East US, South Central US, West Central US, and West Europe, impact may have been experienced to a lesser degree in other regions due to the global nature of ARM.&nbsp;</p>\n<p>This incident also impacted downstream Azure services which depend upon ARM for their internal resource management operations \u2013 including Analysis Services, Azure Container Registry, API Management, App Service, Backup, Bastion, CDN, Center for SAP solutions, Chaos Studio, Data Factory, Database for MySQL flexible servers, Database for PostgreSQL, Databricks, Device Update for IoT Hub, Event Hubs, Front Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL Database, Storage, Synapse Analytics, and Virtual Machines.&nbsp;</p>\n<p>In several cases, data plane impact on downstream Azure services was the result of dependencies on ARM for retrieval of Role Based Access Control (RBAC) data (see: <a href=\"https://learn.microsoft.com/azure/role-based-access-control/overview\" target=\"_blank\">https://learn.microsoft.com/azure/role-based-access-control/overview</a>). For example, services including Storage, Key Vault, Event Hub, and Service Bus rely on ARM to download RBAC authorization policies. During this incident, these services were unable to retrieve updated RBAC information and once the cached data expired these services failed, rejecting incoming requests in the absence of up-to-date access policies. In addition, several internal offerings depend on ARM to support on-demand capacity and configuration changes, leading to degradation and failure when ARM was unable to process their requests.&nbsp;</p>\n<p><strong>What went wrong and why?&nbsp;</strong></p>\n<p>In June 2020, ARM deployed a private preview integration with Entra Continuous Access Evaluation (see: <a href=\"https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation\" target=\"_blank\">https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation</a>). This feature is to support continuous access evaluation for ARM, and was only enabled for a small set of tenants and private preview customers. Unbeknownst to us, this preview feature of the ARM CAE implementation contained a latent code defect that caused issues when authentication to Entra failed. The defect would cause ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview.&nbsp;</p>\n<p>On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview. This triggered the latent code defect and caused ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup.\u202fARM nodes restart periodically by design, to account for automated recovery from transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks.&nbsp;</p>\n<p>Due to these ongoing node restarts and failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Eventually this led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop (increased load resulted in increased timeouts, leading to increased retries and a corresponding further increase in load) and led to a rapid drop in availability. Over time, this impact was experienced in additional regions \u2013 predominantly affecting East US, South Central US, Central US, West Central US, and West Europe.&nbsp;</p>\n<p><strong>How did we respond?&nbsp;</strong></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. By 04:25 UTC we had correlated the preview feature to the ongoing impact. We mitigated by making a configuration change to disable the feature. The mitigation began to rollout at 04:51 UTC, and ARM recovered in all regions except West Europe by 05:30 UTC.&nbsp;</p>\n<p>The recovery in West Europe was slowed because of a retry storm from failed ARM calls, which increased traffic in West Europe by over 20x, causing CPU spikes on our ARM instances. Because most of this traffic originated from trusted internal systems, by default we allowed it to bypass throughput restrictions which would have normally throttled such traffic. We increased throttling of these requests in West Europe which eventually alleviated our CPUs and enabled ARM to recover in the region by 08:58 UTC, at which point the underlying ARM incident was fully mitigated.&nbsp;</p>\n<p>The vast majority of downstream Azure services recovered shortly thereafter. Specific to Key Vault, we identified a latent bug which resulted in application crashes when latency to ARM from the Key Vault data plane was persistently high. This extended the impact for Vaults in East US and West Europe, beyond the vaults that opted into Azure RBAC.&nbsp;</p>\n<ul><li>20 January 2024 @ 21:00 UTC \u2013 An internal maintenance process made a configuration change to an internal tenant enrolled in the CAE private preview.&nbsp;</li><li>20 January 2024 @ 21:16 UTC \u2013 First ARM roles start experiencing startup failures, but no customer impact as ARM still has sufficient capacity to serve requests.&nbsp;</li><li>21 January 2024 @ 01:30 UTC \u2013 Initial customer impact due to continued capacity loss in several large ARM regions.&nbsp;</li><li>21 January 2024 @ 01:59 UTC \u2013 Monitoring detected additional failures in the ARM service, and on-call engineers began immediate investigation.&nbsp;</li><li>21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.&nbsp;</li><li>21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.&nbsp;</li><li>21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.&nbsp;</li><li>21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.&nbsp;</li><li>21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.&nbsp;</li><li>21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.&nbsp;</li><li>21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.&nbsp;</li><li>21 January 2024 @ 05:30 UTC \u2013 ARM recovered in all regions except West Europe.&nbsp;</li><li>21 January 2024 @ 08:58 UTC \u2013 ARM recovered in West Europe, mitigating vast majority of customer impact beyond specific services who took more time to recover.&nbsp;</li><li>21 January 2024 @ 09:28 UTC \u2013 Key Vault recovered instances in West Europe by adding new scale sets to replace the VMs that had crashed due to the code bug.&nbsp;</li></ul>\n<p><strong>How are we making incidents like this less likely or less impactful?&nbsp;</strong></p>\n<p></p><ul><li>Our ARM team have already disabled the preview feature through a configuration update. (Completed)&nbsp;</li><li>We have offboarded all tenants from the CAE private preview, as a precaution. (Completed)&nbsp;</li><li>Our Entra team improved the rollout of that type of per-tenant configuration change to wait for multiple input signals, including from canary regions. (Completed)&nbsp;</li><li>Our Key Vault team has fixed the code that resulted in applications crashing when they were unable to refresh their RBAC caches. (Completed)&nbsp;</li><li>We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will audit dependencies in role startup logic to de-risk scenarios like this one. (Estimated completion: February 2024)&nbsp;</li><li>Our ARM team will leverage Azure Front Door to dynamically distribute traffic for protection against retry storm or similar events. (Estimated completion: February 2024)&nbsp;</li><li>We are improving monitoring signals on role crashes for reduced time spent on identifying the cause(s), and for earlier detection of availability impact. (Estimated completion: February 2024)&nbsp;</li><li>Our Key Vault, Service Bus and Event Hub teams will migrate to a more robust implementation of the Azure RBAC system that no longer relies on ARM and is regionally isolated with standardized implementation. (Estimated completion: February 2024)&nbsp;</li><li>Our Container Registry team are building a solution to detect and auto-fix stale network connections, to recover more quickly from incidents like this one. (Estimated completion: February 2024)&nbsp;</li><li>Finally, our Key Vault team are adding better fault injection tests and detection logic for RBAC downstream dependencies. (Estimated completion: March 2024).&nbsp;</li></ul><p><strong>How can we make our incident communications more useful?&nbsp;</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey:\u202f<a href=\"https://aka.ms/AzPIR/NKRF-1TG\" target=\"_blank\">https://aka.ms/AzPIR/NKRF-1TG</a></p><p></p>",
                                    "updateDateTime": "2024-01-31T11:02:35.2143784Z"
                                },
                                {
                                    "summary": "<p>This is our \"Preliminary\" PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p><br></p>\n<p>Between 01:57 and 08:58 UTC on 21 January 2024, customers attempting to leverage Azure Resource Manager (ARM) may have experienced issues when performing resource management operations. This impacted ARM calls that were made via Azure CLI, Azure PowerShell and the Azure portal. This also impacted downstream Azure services, which depend upon ARM for their internal resource management operations. While the impact was predominantly experienced in East US, South Central US, Central US, West Central US, and West Europe, due to the global nature of ARM impact may have been experienced to a lesser degree in other regions.</p>\n<p><br></p>\n<p><strong>What do we know so far?</strong></p>\n<p><br></p>\n<p>In June 2020, ARM deployed a feature in preview, to support continuous access evaluation (https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation), which was only enabled for a small set of tenants. Unbeknownst to us, this preview feature contained a latent code defect. This caused ARM nodes to fail on startup whenever ARM could not authenticate to an Entra tenant enrolled in the preview. On 21 January 2024, an internal maintenance process made a configuration change to an internal tenant which was enrolled in this preview . This triggered the latent code defect and caused any ARM nodes, which are designed to restart periodically, to fail repeatedly upon startup. The reason that ARM nodes restart periodically is due to transient changes in the underlying platform, and to protect against accidental resource exhaustion such as memory leaks. Due to these failed startups, ARM began experiencing a gradual loss in capacity to serve requests. Over time, this impact spread to additional regions, predominantly affecting East US, South Central US, Central US, West Central US, and West Europe. Eventually this loss of capacity led to an overwhelming of the remaining ARM nodes, which created a negative feedback loop and led to a rapid drop in availability.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p><br></p>\n<p>At 01:59 UTC, our monitoring detected a decrease in availability, and we began an immediate investigation. Automated communications to a subset of impacted customers began shortly thereafter and, as impact to additional regions became better understood, we decided to communicate publicly via the Azure Status page. The causes of the issue were understood by 04:25 UTC. We mitigated impact by making a configuration change to disable the preview feature. The mitigation began roll out at 04:51 UTC, and all regions except West Europe were recovered by 05:30 UTC. The recovery of West Europe was slowed because of a retry storm from failed calls, which intensified traffic in West Europe. We increased throttling of certain requests in West Europe which eventually enabled its recovery by 08:58 UTC, at which point all customer impact was fully mitigated.</p>\n<p><br></p>\n<p>\u2022    21 January 2024 @ 01:59 UTC \u2013 Monitoring detected decrease in availability for the ARM service, and on-call engineers began immediate investigation.</p>\n<p>\u2022    21 January 2024 @ 02:23 UTC \u2013 Automated communication sent to impacted customers started.</p>\n<p>\u2022   21 January 2024 @ 03:04 UTC \u2013 Additional ARM impact was detected in East US and West Europe.</p>\n<p>\u2022    21 January 2024 @ 03:24 UTC \u2013 Due to additional impact identified in other regions, we raised the severity of the incident, and engaged additional teams to assist in troubleshooting.</p>\n<p>\u2022  21 January 2024 @ 03:30 UTC \u2013 Additional ARM impact was detected in South Central US.</p>\n<p>\u2022   21 January 2024 @ 03:57 UTC \u2013 We posted broad communications via the Azure Status page.</p>\n<p>\u2022 21 January 2024 @ 04:25 UTC \u2013 The causes of impact were understood, and a mitigation strategy was developed.</p>\n<p>\u2022    21 January 2024 @ 04:51 UTC \u2013 We began the rollout of this configuration change to disable the preview feature.</p>\n<p>\u2022 21 January 2024 @ 05:30 UTC \u2013 All regions except West Europe were recovered.</p>\n<p>\u2022    21 January 2024 @ 08:58 UTC \u2013 West Europe recovered, fully mitigating all customer impact.</p>\n<p><br></p>\n<p><strong>What happens next?</strong></p>\n<p><br></p>\n<p>\u2022  We have already disabled the preview feature through a configuration update. (Completed)</p>\n<p>\u2022    We are gradually rolling out a change to proceed with node restart when a tenant-specific call fails. (Estimated completion: February 2024)</p>\n<p>\u2022 After our internal retrospective is completed (generally within 14 days) we will publish a \"Final\" PIR with additional details/learnings.</p>\n<p><br></p>\n<p><strong>How can we make our incident communications more useful?</strong></p>\n<p><br></p>\n<p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22TrackingID%22:%22NKRF-1TG%22%7D\" target=\"_blank\">3-question survey</a>.</p>",
                                    "updateDateTime": "2024-01-24T11:01:12.8698087Z"
                                },
                                {
                                    "summary": "<p><strong>Summary of Impact:</strong> Between 01:25 UTC and 09:45 UTC on 21 Jan 2024, you were identified as a customer using Service Bus, Event Hub and / or Azure Relay who may have experienced performance issues with resources hosted in this region.</p>\n<p><strong>Preliminary Root-Cause:</strong> We determined that Azure Resource Manager experienced an outage and impacted our service, resulting in the failure to perform service management operations services.</p>\n<p><strong>Mitigation:</strong> Once the incident with the dependent service was resolved, our services recovered.&nbsp;Having monitored for some time we can confirm that service functionality has been restored.&nbsp;</p>\n<p><strong>Next Steps: </strong>We will follow up in 3 days with a preliminary Post Incident Report (PIR), which will cover the initial root cause and repair items. We'll follow that up 14 days later with a final PIR where we will share a deep dive into the incident.</p>",
                                    "updateDateTime": "2024-01-21T15:27:20.7243117Z"
                                }
                            ]
                        }
                    ]
                }
            ],
            "recommendedActions": {},
            "isHIR": false,
            "priority": 18,
            "lastUpdateTime": "2024-01-31T11:02:35.2143784Z"
        }
    },


    {
        "id": "Microsoft.ResourceHealth/events/M_8K-5ZZ",
        "name": "M_8K-5ZZ",
        "type": "Microsoft.ResourceHealth/events",
        "properties": {
            "eventType": "ServiceIssue",
            "eventSource": "ServiceHealth",
            "status": "Resolved",
            "title": "No Impact - Microsoft Defender for Cloud - Errors when attempting to use MDC API requests",
            "summary": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
            "description": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
            "platformInitiated": true,
            "header": "Your service might have been impacted by an Azure service issue",
            "level": "Warning",
            "eventLevel": "Warning",
            "externalIncidentId": "440100976",
            "impactStartTime": "2023-11-09T11:04:28.93Z",
            "impactMitigationTime": "2023-11-09T17:20:34Z",
            "impact": [
                {
                    "impactedService": "Microsoft Defender for Cloud",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Global",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2023-11-09T17:28:54.8058439Z",
                            "updates": [
                                {
                                    "summary": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
                                    "updateDateTime": "2023-11-09T17:28:54.8058439Z"
                                },
                                {
                                    "summary": "<p>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</p>",
                                    "updateDateTime": "2023-11-09T17:28:15.6172497Z"
                                },
                                {
                                    "summary": "<p><strong>Impact Statement:</strong> Starting at 21:00 UTC on 09 Nov 2023, you have been identified as a customer using Microsoft Defender for Cloud who may experience failures or high latency when attempting to use MDC API requests (e.g., retrieving the security alerts list). Additionally, there may be issues when trying to open the Microsoft Defender for Cloud Security Alert Details blade in the Azure Portal UI.</p>\n<p>&nbsp;</p>\n<p><strong>Current Status:</strong>&nbsp;We suspect that downstream impact from a dependent backend service has caused disruptions in the service. We are continuing our investigation to determine mitigation workflows. The next update will be provided in 60 minutes or as events warrant.</p>\n<p>&nbsp;</p>",
                                    "updateDateTime": "2023-11-09T15:49:13.5624549Z"
                                },
                                {
                                    "summary": "<p>We are investigating an alert for Microsoft Defender for Cloud. We will provide more information as it becomes available.&nbsp;</p>\n<p><br></p>",
                                    "updateDateTime": "2023-11-09T15:18:14.6563685Z"
                                }
                            ]
                        }
                    ]
                }
            ],
            "recommendedActions": {},
            "isHIR": false,
            "priority": 18,
            "lastUpdateTime": "2023-11-09T17:28:54.8058439Z"
        }
    },

    {
        "id": "Microsoft.ResourceHealth/events/M_8K-5ZZ",
        "name": "M_8K-5ZZ",
        "type": "Microsoft.ResourceHealth/events",
        "properties": {
            "eventType": "ServiceIssue",
            "eventSource": "ServiceHealth",
            "status": "Resolved",
            "title": "No Impact - Microsoft Defender for Cloud - Errors when attempting to use MDC API requests",
            "summary": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
            "description": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
            "platformInitiated": true,
            "header": "Your service might have been impacted by an Azure service issue",
            "level": "Warning",
            "eventLevel": "Warning",
            "externalIncidentId": "440100976",
            "impactStartTime": "2023-11-09T11:04:28.93Z",
            "impactMitigationTime": "2023-11-09T17:20:34Z",
            "impact": [
                {
                    "impactedService": "Microsoft Defender for Cloud",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Global",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2023-11-09T17:28:54.8058439Z",
                            "updates": [
                                {
                                    "summary": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
                                    "updateDateTime": "2023-11-09T17:28:54.8058439Z"
                                },
                                {
                                    "summary": "<p>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</p>",
                                    "updateDateTime": "2023-11-09T17:28:15.6172497Z"
                                },
                                {
                                    "summary": "<p><strong>Impact Statement:</strong> Starting at 21:00 UTC on 09 Nov 2023, you have been identified as a customer using Microsoft Defender for Cloud who may experience failures or high latency when attempting to use MDC API requests (e.g., retrieving the security alerts list). Additionally, there may be issues when trying to open the Microsoft Defender for Cloud Security Alert Details blade in the Azure Portal UI.</p>\n<p>&nbsp;</p>\n<p><strong>Current Status:</strong>&nbsp;We suspect that downstream impact from a dependent backend service has caused disruptions in the service. We are continuing our investigation to determine mitigation workflows. The next update will be provided in 60 minutes or as events warrant.</p>\n<p>&nbsp;</p>",
                                    "updateDateTime": "2023-11-09T15:49:13.5624549Z"
                                },
                                {
                                    "summary": "<p>We are investigating an alert for Microsoft Defender for Cloud. We will provide more information as it becomes available.&nbsp;</p>\n<p><br></p>",
                                    "updateDateTime": "2023-11-09T15:18:14.6563685Z"
                                }
                            ]
                        }
                    ]
                },

                {
                    "impactedService": "Microsoft Defender for Cloud",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Global",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2023-11-09T17:28:54.8058439Z",
                            "updates": [
                                {
                                    "summary": "<p><span>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</span></p>",
                                    "updateDateTime": "2023-11-09T17:28:54.8058439Z"
                                },
                                {
                                    "summary": "<p>At 21:00 UTC on 09 Nov 2023, we received a monitoring alert for a possible issue with Microsoft Defender for Cloud. We have concluded our investigation of the alert, confirmed that all services are healthy, and a service incident did not occur. We apologize for any inconvenience caused.</p>",
                                    "updateDateTime": "2023-11-09T17:28:15.6172497Z"
                                },
                                {
                                    "summary": "<p><strong>Impact Statement:</strong> Starting at 21:00 UTC on 09 Nov 2023, you have been identified as a customer using Microsoft Defender for Cloud who may experience failures or high latency when attempting to use MDC API requests (e.g., retrieving the security alerts list). Additionally, there may be issues when trying to open the Microsoft Defender for Cloud Security Alert Details blade in the Azure Portal UI.</p>\n<p>&nbsp;</p>\n<p><strong>Current Status:</strong>&nbsp;We suspect that downstream impact from a dependent backend service has caused disruptions in the service. We are continuing our investigation to determine mitigation workflows. The next update will be provided in 60 minutes or as events warrant.</p>\n<p>&nbsp;</p>",
                                    "updateDateTime": "2023-11-09T15:49:13.5624549Z"
                                },
                                {
                                    "summary": "<p>We are investigating an alert for Microsoft Defender for Cloud. We will provide more information as it becomes available.&nbsp;</p>\n<p><br></p>",
                                    "updateDateTime": "2023-11-09T15:18:14.6563685Z"
                                }
                            ]
                        }
                    ]
                }
            ],
            "recommendedActions": {},
            "isHIR": false,
            "priority": 18,
            "lastUpdateTime": "2023-11-09T17:28:54.8058439Z"
        }
    },


    {
        "id": "Microsoft.ResourceHealth/events/BN60-HSZ",
        "name": "BN60-HSZ",
        "type": "Microsoft.ResourceHealth/events",
        "properties": {
            "eventType": "ServiceIssue",
            "eventSource": "ServiceHealth",
            "status": "Resolved",
            "title": "Microsoft Entra ID - South Africa West and South Africa North - Mitigated",
            "summary": "<p><strong>Impact Statement</strong>: Between 10:30 UTC and 11:57 UTC on 14 March 2024&nbsp;you were identified among a subset of customers using&nbsp;Microsoft Entra ID in South Africa North and West who may have experienced&nbsp;some latency and sign-in failures. We determined that a networking issue that has resulted in higher-than-expected latency for users whose resources are hosted in or connect with South Africa North and West caused this issue.</p>",
            "description": "<p><strong>Impact Statement</strong>: Between 10:30 UTC and 11:57 UTC on 14 March 2024&nbsp;you were identified among a subset of customers using&nbsp;Microsoft Entra ID in South Africa North and West who may have experienced&nbsp;some latency and sign-in failures. We determined that a networking issue that has resulted in higher-than-expected latency for users whose resources are hosted in or connect with South Africa North and West caused this issue.</p>\n<p><strong>Mitigation</strong>: The automated monitoring system took out the affected capacity from the rotation so that other regions that were healthy could handle the requests. We validated through our monitoring that the issue was mitigated.</p>\n<p><strong>Next Steps</strong>: We will continue to monitor the service and will further investigate the network connection issue to establish the full root cause to help prevent future occurrences. Stay informed about Azure service issues by creating custom service health alerts:&nbsp;<a href=\"https://aka.ms/ash-videos\" target=\"_blank\">https://aka.ms/ash-videos</a>&nbsp;for video tutorials and&nbsp;<a href=\"https://aka.ms/ash-alerts\" target=\"_blank\">https://aka.ms/ash-alerts</a>&nbsp;for how-to documentation.</p>",
            "platformInitiated": true,
            "header": "Your service might have been impacted by an Azure service issue",
            "level": "Warning",
            "eventLevel": "Informational",
            "externalIncidentId": "482394294",
            "impactStartTime": "2024-03-14T10:43:49.937Z",
            "impactMitigationTime": "2024-03-14T11:57:00Z",
            "impact": [
                {
                    "impactedService": "Azure Active Directory",
                    "impactedRegions": [
                        {
                            "impactedRegion": "Global",
                            "status": "Resolved",
                            "impactedSubscriptions": [
                                "d8732e82-febd-4b92-b1ed-8fbce80a9ad8"
                            ],
                            "impactedTenants": [],
                            "lastUpdateTime": "2024-03-14T20:03:01.1698529Z",
                            "updates": [
                                {
                                    "summary": "<p><strong>Impact Statement</strong>: Between 10:30 UTC and 11:57 UTC on 14 March 2024&nbsp;you were identified among a subset of customers using&nbsp;Microsoft Entra ID in South Africa North and West who may have experienced&nbsp;some latency and sign-in failures. We determined that a networking issue that has resulted in higher-than-expected latency for users whose resources are hosted in or connect with South Africa North and West caused this issue.</p>\n<p><strong>Mitigation</strong>: The automated monitoring system took out the affected capacity from the rotation so that other regions that were healthy could handle the requests. We validated through our monitoring that the issue was mitigated.</p>\n<p><strong>Next Steps</strong>: We will continue to monitor the service and will further investigate the network connection issue to establish the full root cause to help prevent future occurrences. Stay informed about Azure service issues by creating custom service health alerts:&nbsp;<a href=\"https://aka.ms/ash-videos\" target=\"_blank\">https://aka.ms/ash-videos</a>&nbsp;for video tutorials and&nbsp;<a href=\"https://aka.ms/ash-alerts\" target=\"_blank\">https://aka.ms/ash-alerts</a>&nbsp;for how-to documentation.</p>",
                                    "updateDateTime": "2024-03-14T20:03:01.1698529Z"
                                },
                                {
                                    "summary": "<p>Engineers are investigating an alert for Entra ID in South Africa North and South Africa West. More information will be provided in 60 minutes or as events warrant. </p>",
                                    "updateDateTime": "2024-03-14T15:09:35.5280634Z"
                                }
                            ]
                        }
                    ]
                }
            ],
            "recommendedActions": {},
            "isHIR": false,
            "priority": 19,
            "lastUpdateTime": "2024-03-14T20:03:01.1698529Z"
        }
    }
]